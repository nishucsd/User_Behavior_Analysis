{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import dateutil.parser\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For Mac OS X and Linux\n",
    "#spark_path = \"/opt/spark\"\n",
    "# For Windows 7+\n",
    "spark_path = \"C:/opt/spark\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.9-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sc = SparkContext(\"local\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './Data//'\n",
    "fileList = ['e1.txt','e2.txt','e3.txt','e4.txt','e5.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x4e49710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-01-07-44-25_2015-06-02-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-02-07-44-25_2015-06-03-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-03-07-44-25_2015-06-04-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-04-07-44-25_2015-06-05-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-05-07-44-25_2015-06-06-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-06-07-44-25_2015-06-07-07-44-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-07-21-51-37_2015-06-08-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-08-21-51-37_2015-06-09-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-09-21-51-37_2015-06-10-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-10-21-51-37_2015-06-11-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-11-21-51-37_2015-06-12-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-12-21-51-37_2015-06-13-21-51-37',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-13-22-18-01_2015-06-14-22-18-01',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-15-07-26-25_2015-06-16-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-16-07-26-25_2015-06-17-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-17-07-26-25_2015-06-18-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-18-07-26-25_2015-06-19-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-19-07-26-25_2015-06-20-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-20-07-26-25_2015-06-21-07-26-25',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-21-22-59-11_2015-06-22-22-59-11',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-22-22-59-21_2015-06-23-22-59-21',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-23-22-59-24_2015-06-24-22-59-24',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-24-22-59-26_2015-06-25-22-59-26',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-25-22-59-26_2015-06-26-22-59-26',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-26-22-59-27_2015-06-27-22-59-27',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-27-23-26-40_2015-06-28-23-26-40',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-29-05-51-38_2015-06-30-05-51-38',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-06-30-05-51-44_2015-07-01-05-51-44',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-07-01-05-51-44_2015-07-02-05-51-44',\n",
       " 'C:\\\\Users\\\\nisagarw\\\\Data\\\\poc_data_sorted2\\\\sats_events_2015-07-02-05-52-06_2015-07-03-05-52-06']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "import socket, struct\n",
    "\n",
    "def is_json(myjson):\n",
    "        try:\n",
    "            json_object = ujson.loads(myjson)\n",
    "            p1 = (json_object[\"User\"][\"User\"])\n",
    "        except ValueError as e:\n",
    "            return False\n",
    "        except KeyError as e2:\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    return raw_json.filter(lambda t : is_json(t)).map(lambda myjson : ujson.loads(myjson))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x5bac7b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.set('spark.executor.memory','32g').set('spark.driver.memory','32g').set('spark.driver.maxResultsSize','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0  done!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c5d9ab189628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msafe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrnd_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Department\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'R&D'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SourceIP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfaulty_users\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrnd_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SourceIP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaulty_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaulty_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/opt/spark/python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mC:\\opt\\spark\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nisagarw\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in files:\n",
    "    print(\"file:\",k, \" done!\")\n",
    "    k+=1\n",
    "    content_rdd = sc.textFile(i)\n",
    "    safe = safe_parse(content_rdd)\n",
    "    rnd_logs = safe.filter(lambda x: x[\"User\"][\"Department\"] == 'R&D' and len(x['SourceIP']) >1) \n",
    "    faulty_users  = rnd_logs.map(lambda x: ((x[\"User\"][\"User\"],x['SourceIP']),1)) \\\n",
    "                    .reduceByKey(lambda x,y: x+y).map(lambda x: (x[0][0],(x[0][1],x[1])))   \\\n",
    "                    .reduceByKey(lambda x,y: x+y).collect()\n",
    "    if(faulty_users):\n",
    "        print(dict(faulty_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faulty_users  = rnd_logs.map(lambda x: ((x[\"User\"][\"User\"],x['SourceIP'].split('.')[:3] ),1)).reduceByKey(lambda x,y: x+y) \\\n",
    "                    .map(lambda (x,y): (x[0],(x[1],y))).reduceByKey(lambda x,y: x+y).filter(lambda (x,y): len(y)>2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = files[0]\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'EventCollector': 'Imperva Inc.',\n",
       "  'EventDescription': {'BindVariables': '',\n",
       "   'DataStore': 'COE_PCRS_951/BANK_ORA3',\n",
       "   'EventAffectedCount': '62',\n",
       "   'EventApplicationName': 'Default Oracle Application',\n",
       "   'IsSensitive': True,\n",
       "   'Query': 'SELECT \"COUNTRY\", \"ACCOUNT\", \"ZIP\" FROM \"BANK\".\"BANK_ORA3\", \"BANK\".\"BANK_LIN4\"',\n",
       "   'SensitiveDataDomains': [{'columns': [{'columnName': 'ACCOUNT',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'China_NationalID'},\n",
       "    {'columns': [{'columnName': 'COUNTRY',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'DriverLicence_Canada'},\n",
       "    {'columns': [{'columnName': 'ZIP',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'Passport_MachineReadable'}],\n",
       "   'UserGroup': 'Default oracle group'},\n",
       "  'EventMessageType': 'CEF',\n",
       "  'EventUid': '729186814',\n",
       "  'RawEventMessage': '<13>Apr 23 10:06:18 quickstart cloudera: CEF: 0|Imperva Inc.|SecureSphere|10.5.0.4_0|Audit|Audit.DAM|Informative|dst=10.17.40.25 dpt=1521 duser=Zamora,  Teegan W. src=10.17.0.102 spt=43570 proto=TCP rt=01 June 2015 06:14:25, cat=Audit Infa DB Audit cs1Label=Policy cs2=Database Server Group cs2Label=ServerGroup cs3=Oracle Services cs3Label=ServiceName cs4=Default Oracle Application cs4Label=ApplicationName cs5=1086628793948 cs5Label=EventId cs6=Query cs6Label=EventType cs7=Default oracle group cs7Label=UserGroup cs8=True cs8Label=UserAuthenticated cs9= cs9Label=ApplicationUser cs10=jdbc thin client cs10Label=SourceApplication cs11=oracle cs11Label=OSUser cs12=pbox-montreal cs12Label=HostName cs13=COE_PCRS_951/BANK_ORA3 cs13Label=Database cs14=hr cs14Label=Schema cs15= cs15Label=RawQuery cs16=SELECT \"COUNTRY\", \"ACCOUNT\", \"ZIP\" FROM \"BANK\".\"BANK_ORA3\", \"BANK\".\"BANK_LIN4\" cs16Label=ParsedQuery cs17= cs17Label=BindVariables cs18= cs18Label=SQLError cs19=0 cs19Label=ResponseSize cs20=57 cs20Label=ResponseTime cs21=62 cs21Label=AffectedRows\\n',\n",
       "  'SourceEmail': '',\n",
       "  'SourceHostname': '',\n",
       "  'SourceIP': '10.77.1.57',\n",
       "  'TargetEmail': '',\n",
       "  'TargetHostname': '',\n",
       "  'TargetIP': '10.17.40.25',\n",
       "  'TimeOfEvent': '2015-06-01-06-14-25',\n",
       "  'TimeOfEventUTC': '2015-06-01-00-44-25',\n",
       "  'User': {'Department': 'R&D',\n",
       "   'Location': 'Hyderab',\n",
       "   'User': 'Zamora,  Teegan W.',\n",
       "   'UserGroup': 'Analyst'}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(i)\n",
    "safe = safe_parse(content_rdd)\n",
    "\n",
    "temp = safe.take(1)[0]['RawEventMessage']\n",
    "safe.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_src_ip(x):\n",
    "    temp=x['RawEventMessage']\n",
    "    ip = re.search('src=(.+?) ', temp).group(1)\n",
    "    if ip.endswith('|'):\n",
    "        return ip[:-1]\n",
    "    else:\n",
    "        return ip\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.2.2.343'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = '10.2.2.343|'\n",
    "ip[:-1]\n",
    "# if ip.endswith('|'):\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'EventCollector': 'Imperva Inc.',\n",
       "  'EventDescription': {'BindVariables': '',\n",
       "   'DataStore': 'COE_PCRS_951/BANK_ORA3',\n",
       "   'EventAffectedCount': '62',\n",
       "   'EventApplicationName': 'Default Oracle Application',\n",
       "   'IsSensitive': True,\n",
       "   'Query': 'SELECT \"COUNTRY\", \"ACCOUNT\", \"ZIP\" FROM \"BANK\".\"BANK_ORA3\", \"BANK\".\"BANK_LIN4\"',\n",
       "   'SensitiveDataDomains': [{'columns': [{'columnName': 'ACCOUNT',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'China_NationalID'},\n",
       "    {'columns': [{'columnName': 'COUNTRY',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'DriverLicence_Canada'},\n",
       "    {'columns': [{'columnName': 'ZIP',\n",
       "       'schemaName': 'BANK',\n",
       "       'tableName': 'BANK_ORA3'}],\n",
       "     'dataDomainName': 'Passport_MachineReadable'}],\n",
       "   'UserGroup': 'Default oracle group'},\n",
       "  'EventMessageType': 'CEF',\n",
       "  'EventUid': '729186814',\n",
       "  'RawEventMessage': '<13>Apr 23 10:06:18 quickstart cloudera: CEF: 0|Imperva Inc.|SecureSphere|10.5.0.4_0|Audit|Audit.DAM|Informative|dst=10.17.40.25 dpt=1521 duser=Zamora,  Teegan W. src=10.17.0.102 spt=43570 proto=TCP rt=01 June 2015 06:14:25, cat=Audit Infa DB Audit cs1Label=Policy cs2=Database Server Group cs2Label=ServerGroup cs3=Oracle Services cs3Label=ServiceName cs4=Default Oracle Application cs4Label=ApplicationName cs5=1086628793948 cs5Label=EventId cs6=Query cs6Label=EventType cs7=Default oracle group cs7Label=UserGroup cs8=True cs8Label=UserAuthenticated cs9= cs9Label=ApplicationUser cs10=jdbc thin client cs10Label=SourceApplication cs11=oracle cs11Label=OSUser cs12=pbox-montreal cs12Label=HostName cs13=COE_PCRS_951/BANK_ORA3 cs13Label=Database cs14=hr cs14Label=Schema cs15= cs15Label=RawQuery cs16=SELECT \"COUNTRY\", \"ACCOUNT\", \"ZIP\" FROM \"BANK\".\"BANK_ORA3\", \"BANK\".\"BANK_LIN4\" cs16Label=ParsedQuery cs17= cs17Label=BindVariables cs18= cs18Label=SQLError cs19=0 cs19Label=ResponseSize cs20=57 cs20Label=ResponseTime cs21=62 cs21Label=AffectedRows\\n',\n",
       "  'SourceEmail': '',\n",
       "  'SourceHostname': '',\n",
       "  'SourceIP': '10.77.1.57',\n",
       "  'TargetEmail': '',\n",
       "  'TargetHostname': '',\n",
       "  'TargetIP': '10.17.40.25',\n",
       "  'TimeOfEvent': '2015-06-01-06-14-25',\n",
       "  'TimeOfEventUTC': '2015-06-01-00-44-25',\n",
       "  'User': {'Department': 'R&D',\n",
       "   'Location': 'Hyderab',\n",
       "   'User': 'Zamora,  Teegan W.',\n",
       "   'UserGroup': 'Analyst'}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = safe.filter(lambda x: x['EventCollector'] == 'Imperva Inc.').take(1)\n",
    "arr\n",
    "# get_src_ip(safe.take(4)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Admin': 18,\n",
       "             'Finance': 13,\n",
       "             'HR': 14,\n",
       "             'Legal': 17,\n",
       "             'R&D': 14,\n",
       "             'Sale': 22,\n",
       "             'Support': 25})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe.map(lambda x : (  x['User']['Department'],x['User']['User']) ).distinct().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Account_Status': 937,\n",
       "             'Age': 102,\n",
       "             'China_NationalID': 6323,\n",
       "             'Country': 1649,\n",
       "             'CreditCard_AMEX': 115,\n",
       "             'CreditCard_Visa': 667,\n",
       "             'Date_AllFormats': 905,\n",
       "             'Denmark_NationalID': 775,\n",
       "             'DriverLicence_Canada': 1370,\n",
       "             'DriversLicence_GBR': 794,\n",
       "             'Finland_NationalID': 393,\n",
       "             'Geocode_LatitudeLongtitude': 467,\n",
       "             'GreatBritian_NINO': 738,\n",
       "             'IBAN': 424,\n",
       "             'India_NationalID': 2388,\n",
       "             'Italy_FiscalCode': 56,\n",
       "             'JobPosition': 154,\n",
       "             'Passport_DEU_MR': 1044,\n",
       "             'Passport_India': 200,\n",
       "             'Passport_MachineReadable': 804,\n",
       "             'PhoneNumber': 640,\n",
       "             'Postcode': 170,\n",
       "             'SouthAfrica_NationalID': 174})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe.filter(lambda x: x['EventDescription']['IsSensitive'] == True ).map(lambda x: (x['EventDescription']['SensitiveDataDomains'][0]['dataDomainName'],1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sens_domains(x):\n",
    "    sens_dict = Counter({})\n",
    "    if x['EventDescription']['IsSensitive'] == True :\n",
    "         for i in x['EventDescription']['SensitiveDataDomains']:\n",
    "                sens_dict[i['dataDomainName']]=1\n",
    "    \n",
    "    return sens_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-536eed91a193>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msens_domains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-42d8b16528eb>\u001b[0m in \u001b[0;36msens_domains\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msens_domains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msens_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EventDescription'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IsSensitive'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m          \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EventDescription'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SensitiveDataDomains'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0msens_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataDomainName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "sens_domains(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from calendar import timegm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(a,b):\n",
    "    return tuple([i+j for i,j in zip(a,b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "def convert_hour(s):\n",
    "    d = dateutil.parser.parse(s)\n",
    "    return str(int(d.hour/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in files:\n",
    "    print(\"file:\",k, \" done!\")\n",
    "    k+=1\n",
    "    content_rdd = sc.textFile(i)\n",
    "    safe = safe_parse(content_rdd)\n",
    "    rnd_logs = safe.filter(lambda x: x[\"User\"][\"Department\"] == 'R&D' and len(x['SourceIP']) >1) \\\n",
    "                    .map(lambda x: (x[\"User\"][\"User\"],Counter({x['SourceIP']:1})) )\n",
    "    faulty_users  = rnd_logs.reduceByKey(lambda x,y: x+y).filter(lambda x: len(x[1])>1).collect()\n",
    "    if(faulty_users):\n",
    "        print(dict(faulty_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1234'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'gfgfdAAA1234ZZZuijjk'\n",
    "\n",
    "m = re.search('AAA(.+?)ZZZu', text).group(1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0  done!\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 523.0 failed 1 times, most recent failure: Lost task 0.0 in stage 523.0 (TID 1444, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-114-e21e0797710c>\", line 7, in <lambda>\n  File \"<ipython-input-112-9c71f6646db0>\", line 3, in get_src_ip\nAttributeError: 'NoneType' object has no attribute 'group'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-114-e21e0797710c>\", line 7, in <lambda>\n  File \"<ipython-input-112-9c71f6646db0>\", line 3, in get_src_ip\nAttributeError: 'NoneType' object has no attribute 'group'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-e21e0797710c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msafe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrnd_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Department\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'R&D'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_src_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SourceIP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mconvert_hour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TimeOfEventUTC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msens_domains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfaulty_users\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrnd_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaulty_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaulty_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/opt/spark/python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 523.0 failed 1 times, most recent failure: Lost task 0.0 in stage 523.0 (TID 1444, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-114-e21e0797710c>\", line 7, in <lambda>\n  File \"<ipython-input-112-9c71f6646db0>\", line 3, in get_src_ip\nAttributeError: 'NoneType' object has no attribute 'group'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor310.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:/opt/spark/python\\pyspark\\rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\opt\\spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-114-e21e0797710c>\", line 7, in <lambda>\n  File \"<ipython-input-112-9c71f6646db0>\", line 3, in get_src_ip\nAttributeError: 'NoneType' object has no attribute 'group'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in files:\n",
    "    print(\"file:\",k, \" done!\")\n",
    "    k+=1\n",
    "    content_rdd = sc.textFile(i)\n",
    "    safe = safe_parse(content_rdd)\n",
    "    rnd_logs = safe.filter(lambda x: x[\"User\"][\"Department\"] == 'R&D' and len(x['SourceIP']) >1) \\\n",
    "                    .map(lambda x: (x[\"User\"][\"User\"],(Counter({x['SourceIP']:1}), Counter({convert_hour(x['TimeOfEventUTC']):1}), sens_domains(x) )) )          \n",
    "    faulty_users  = rnd_logs.reduceByKey(lambda x,y: combine(x,y)).collect()\n",
    "    if(faulty_users):\n",
    "        print(dict(faulty_users).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file2 =  codecs.open(\"topics3.json\",'w',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0  done!\n",
      "file: 1  done!\n",
      "file: 2  done!\n",
      "file: 3  done!\n",
      "file: 4  done!\n",
      "file: 5  done!\n",
      "file: 6  done!\n",
      "file: 7  done!\n",
      "file: 8  done!\n",
      "file: 9  done!\n",
      "file: 10  done!\n",
      "file: 11  done!\n",
      "file: 12  done!\n",
      "file: 13  done!\n",
      "file: 14  done!\n",
      "file: 15  done!\n",
      "file: 16  done!\n",
      "file: 17  done!\n",
      "file: 18  done!\n",
      "file: 19  done!\n",
      "file: 20  done!\n",
      "file: 21  done!\n",
      "file: 22  done!\n",
      "file: 23  done!\n",
      "file: 24  done!\n",
      "file: 25  done!\n",
      "file: 26  done!\n",
      "file: 27  done!\n",
      "file: 28  done!\n",
      "file: 29  done!\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in files:\n",
    "    print(\"file:\",k, \" done!\")\n",
    "    k+=1\n",
    "    content_rdd = sc.textFile(i)\n",
    "    safe = safe_parse(content_rdd)\n",
    "    \n",
    "    rnd_logs = safe.filter(lambda x: x[\"User\"][\"Department\"] == 'Sale').filter(lambda x: x['EventCollector'] == 'Imperva Inc.').cache()\n",
    "    cate_logs = rnd_logs.map(lambda x: (x[\"User\"][\"User\"],(Counter({x['SourceIP']:1}), Counter({convert_hour(x['TimeOfEventUTC']):1}), sens_domains(x) )) )          \n",
    "    cont_logs = rnd_logs.map(lambda x: (x[\"User\"][\"User\"], ( int(x['EventDescription']['EventAffectedCount']), 0 if x['EventDescription']['IsSensitive'] == False else 1 )))\n",
    "    cat_users  = cate_logs.reduceByKey(lambda x,y: combine(x,y))\n",
    "    cont_users = cont_logs.reduceByKey(lambda x,y: combine(x,y))\n",
    "#     u_feature_vec = cat_users.map(lambda x: (x[0],x[1][0])).collect()\n",
    "    u_feature_vec = cat_users.leftOuterJoin(cont_users).collect()\n",
    "    if(u_feature_vec):\n",
    "#         print(dict(u_feature_vec))\n",
    "        json.dump(dict(u_feature_vec), file2)\n",
    "        file2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r={'df':1,'ty':77}\n",
    "len(Counter(r).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = files[1]\n",
    "line = open(i).readlines()[0]\n",
    "content_rdd = sc.parallelize(line.replace('}{','}\\n{').split('\\n')).cache()\n",
    "content_rdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe = safe_parse(content_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe.map(lambda x: (x[\"User\"][\"Department\"], x[\"User\"][\"User\"])).distinct().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geolite2 import geolite2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnd_logs = safe.filter(lambda x: x[\"User\"][\"Department\"] == 'R&D' and len(x['SourceIP']) >1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = rnd_logs.map(lambda x: x[\"User\"][\"User\"]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = rnd_logs.map(lambda x: (tuple(x['SourceIP'].split('.')[:3]),1) ).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'Mendoza,  Austin D.': (('10', '77', '0'), 26), 'Cherry,  Wynne S.': (('10', '77', '0'), 20), 'Graves,  Kylan I.': (('10', '77', '0'), 28), 'Avila,  Tiger Y.': (('10', '77', '5'), 29), 'Bonner,  Wing P.': (('10', '77', '4'), 1), 'Greer,  Mannix X.': (('10', '77', '1'), 1), 'Wells,  Mary O.': (('10', '77', '3'), 85), 'Washington,  Russell X.': (('10', '77', '0'), 6), 'Pena,  Burton Y.': (('10', '77', '3'), 170)}\n",
    "b = {'Blackburn,  Quyn K.': (('10', '77', '2'), 350), 'Golden,  McKenzie W.': (('10', '77', '4'), 45), 'Bonner,  Wing P.': (('10', '77', '4'), 56), 'Greer,  Mannix X.': (('10', '77', '1'), 325), 'Mendoza,  Austin D.': (('10', '77', '0'), 7), 'Cherry,  Wynne S.': (('10', '77', '0'), 15), 'Zamora,  Teegan W.': (('10', '77', '1'), 327), 'Thornton,  Guinevere L.': (('10', '77', '2'), 65), 'Chandler,  Madeline G.': (('10', '77', '1'), 293), 'Washington,  Russell X.': (('10', '77', '0'), 31), 'Wells,  Mary O.': (('10', '77', '3'), 5), 'Crane,  Shelley K.': (('10', '77', '1'), 480)}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
